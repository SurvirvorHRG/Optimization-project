{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT: Course Optimization for Data Science\n",
    "## Optimization strategies for robust regression\n",
    "\n",
    "\n",
    "Author: Alexandre Gramfort, Rémi Flamary\n",
    "\n",
    "If you have questions or if something is not clear in the text below please contact us\n",
    "by email.\n",
    "\n",
    "## Aim:\n",
    "\n",
    "- Formulate the Mean Absolute Error (MAE) regression model as a Quadratic Program\n",
    "- Derive mathematically and implement the loss and gradients of the Huber model\n",
    "- Implement your own solvers for L1 or squared L2 regularization with: (Accelerated) Proximal gradient descent, Proximal coordinate descent and L-BFGS (only for L2).\n",
    "- Implement your own scikit-learn estimator for the Huber model and test it against the Ridge and Lasso from scikit-learn on some real data.\n",
    "- You are expected to provide clear figures as one could expect from an experiment section in a research paper.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "This work must be done by pairs of students.\n",
    "Each student must send their work before the 31st of January at 23:59, using the moodle platform.\n",
    "This means that **each student in the pair sends the same file**\n",
    "\n",
    "On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called \"Project\".\n",
    "This is where you submit your jupyter notebook file.\n",
    "\n",
    "The name of the file must be constructed as in the next cell\n",
    "\n",
    "### Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "#### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_flamary_remi_and_gramfort_alexandre.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"remi\"\n",
    "ln1 = \"flamary\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"project\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports (install cvxopt if not already done so `!pip install cvxopt`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Why a robust regression model\n",
    "\n",
    "\n",
    "Let us consider the problem of regression from $n$ observations\n",
    "$x_i \\in \\mathbb{R}^{p}$,\n",
    "$1 \\leq i \\leq n$. We aim to learn a function:\n",
    "$$f: x \\in \\mathbb{R}^{p}\\mapsto y\\in\\mathbb{R}$$\n",
    "from the $n$ annotated training samples $(x_{i},y_{i})$ supposed i.i.d. from an unknown probability distribution on $\\mathbb{R}^p \\times \\mathbb{R}$. Once this function is learnt, it will be possible to use it to predict the label $y$ associated to a new sample $x$.\n",
    "\n",
    "The types of model we consider in this project are so-called *robust models* that can deal with samples corrupted by strong artifacts.\n",
    "\n",
    "Let's generate such a dataset in 1D to illustrate the problem when using the squared loss ($\\|\\cdot\\|^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate toy data\n",
    "X, y = make_regression(n_samples=20, n_features=1, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "\n",
    "# Add an outlier\n",
    "X[0, 0] = 2.\n",
    "y[0] = 350\n",
    "\n",
    "# Fit the model\n",
    "t0 = time.perf_counter()\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "print(f\"Time for LinearRegression: {time.perf_counter() - t0:.3f}s\")\n",
    "\n",
    "# Visualize the model\n",
    "x = X[:, 0]\n",
    "y_pred = reg.coef_ * x + reg.intercept_\n",
    "\n",
    "plt.plot(x, y, 'b.')\n",
    "plt.plot(x, y_pred, 'g-', label=\"linear regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 0:</b>\n",
    "     <ul>\n",
    "       <li>Describe the issue you observe and suggest an explanation and a possible solution.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Mean Absolute Error regression\n",
    "\n",
    "The Mean Absolute Error (MAE) regression model reads:\n",
    "\n",
    "$$\n",
    "\\min_{w \\in \\mathbb{R}^p} \\frac{1}{n} \\sum_{i=1}^n \\left|y_i - \\langle w, x_i\n",
    "\\rangle-b \\right| + \\lambda \\frac{1}{2}\\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "where $\\lambda > 0$ is the regularization parameter for quadratic\n",
    "regularization.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b>\n",
    "     <ul>\n",
    "       <li>Reformulate the optimization problem as a standard Quadratic Program of the form :</li></ul>\n",
    "       \n",
    "$$ \\min_{z \\in \\mathbb{R}^p} f(w) = \\frac{1}{2} z^T Q z + c^T z, \\quad \\text{ subject to } Gz\\leq h \\text{ and } Az=b $$\n",
    "       \n",
    "       \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "       <li>Code the solver for the regularized MAE using the QP solver from `cvxopt` (see example <a href=\"https://cvxopt.org/examples/tutorial/qp.html\">here</a>) or another open source QP solver of your choice. Note that for cvxopt`, numpy array have to be converted to cvxopt `matrix`.</li></ul>          \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "def solve_reg_mae(X,y,reg=0):\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    sol = solvers.qp(matrix(Q), matrix(c), matrix(G), matrix(h))\n",
    "    w = np.array(sol['x'])[1:n_features+1,0] # recover w\n",
    "    b = np.array(sol['x'])[0] # recover b (first component of z)\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b>\n",
    "     <ul>\n",
    "       <li>Apply the solver on the toy data and compare it to the previous LinearRegression solution with $\\lambda=0.1$.</li></ul>    \n",
    "     <ul> <li>Discuss the robustness of MAE VS LS.</li></ul>       \n",
    "          <ul> <li>Discuss the computational time of MAE QP solver. What is the number of variable to optimize? How well will it scale  to large datasets?</li></ul>       \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.perf_counter()\n",
    "w_mae, b_mae = solve_reg_mae(X,y,reg=0.1)\n",
    "print(f\"Time for MAE QP solver: {time.perf_counter() - t0:.3f}s\")\n",
    "\n",
    "ypred_mae = X.dot(w_mae)+b_mae\n",
    "\n",
    "plt.plot(x, y, 'b.')\n",
    "plt.plot(x, y_pred, 'g-', label=\"linear regression\")\n",
    "plt.plot(x, ypred_mae, 'r-', label=\"MAE regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Huber Loss\n",
    "\n",
    "One version of the Huber function ($H_\\epsilon : \\mathbb{R} \\rightarrow \\mathbb{R}$) reads:\n",
    "\n",
    "$$\n",
    "    H_\\epsilon (x) = \\left\\{\n",
    "\t\\begin{aligned}\n",
    "\tx^2 & \\quad \\mathrm{ if } \\quad |x| < \\epsilon \\\\\n",
    "    2 \\epsilon |x| - \\epsilon^2 & \\quad \\mathrm{ otherwise }\n",
    "\t\\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Working in a regression setting, the Huber loss between 2 targets $y$ and $y'$ reads:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(y, y') = H_\\epsilon (y - y')\n",
    "$$\n",
    "\n",
    "Here is an implemention of the Huber function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.\n",
    "\n",
    "def huber(x, epsilon=epsilon):\n",
    "    mask = np.abs(x) < epsilon\n",
    "    z = x.copy()\n",
    "    z[mask] = x[mask] ** 2\n",
    "    z[~mask] = 2 * epsilon * np.abs(x[~mask]) - epsilon ** 2\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 4:</b>\n",
    "     <ul>\n",
    "       <li>Plot the Huber function vs. the squared function ($x \\rightarrow x^2$) vs. the absolute value function ($x \\rightarrow |x|$) between -3 and 3 using $\\epsilon = 1$</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 5:</b>\n",
    "     <ul>\n",
    "       <li>Justify the convexity of the Huber function as defined above.</li>\n",
    "       <li>Justify the smoothness of the Huber function as defined above and propose a value of for the Lipschitz constant of its gradient.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 6:</b>\n",
    "     <ul>\n",
    "       <li>Write a function that computes the gradient of the Huber loss.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** You will use the `scipy.optimize.check_grad` function to assess the validity of your result. You will need to test your gradient in both the linear and quadratic regions of the Huber function (not just in one location)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the cost function associated to the empirical risk with some regularization function $\\mathcal{R}$:\n",
    "\n",
    "$$\n",
    "    (\\mathcal{P}_{f,\\mathcal{R}}):\n",
    "\t\\begin{aligned}\n",
    "\t\\min_{w \\in \\mathbb{R}^p, b \\in \\mathbb{R}} \\quad \\frac{1}{n} \\sum_{i=1}^n f(y_i - x_i^\\top w - b) + \\lambda \\mathcal{R}(w) \\enspace ,\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $f$ is a scalar function defining the loss (Huber, squared, absolute etc.). The variable $b$ is the bias or intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 7:</b>\n",
    "     <ul>\n",
    "      <li>Let us consider for $\\mathcal{R}$ either the $\\ell_1$ norm ($\\mathcal{R}_1(w) = \\|w\\|_1 = \\sum_{j=1}^p |w_j|$) or the squared $\\ell_2$ norm ($\\mathcal{R}_2(w) = \\|w\\|_2^2 = \\sum_{j=1}^p w_j^2)$. Justify what optimization strategy among L-BFGS, (proximal-)gradient descent, (proximal-)coordinate descent is readily applicable, depending on the choice of $\\mathcal{R}$ when $f$ is the Huber function as defined above.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 8:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Taking as $f$ the Huber function and the $\\mathcal{R}_2$ regularization function, solve the optimization prolem $(\\mathcal{P}_{H_\\epsilon,\\mathcal{R}_2})$ using the `fmin_l_bfgs_b` function from `scipy.optimize`. You are expected to provide the explicit gradient (fprime parameter) to `fmin_l_bfgs_b`.\n",
    "    </li>\n",
    "    <li>Using the simulated dataset from above, you will check that your solver fixes the problem of the outlier provided that $\\lambda$ is small enough (eg. $\\lambda = 0.01$). Your are expected to make a plot of the regression fit.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "The estimate of $w$ and $b$ should be called `w_hat` and `b_hat`. You will call the regularization parameter $\\lambda$ as `lbda` in the code.\n",
    "\n",
    "To help you we provide you with the function `pobj_l2` that computes the primal objective to minimize. Note that the parameters `w` and `b` are combined in a single array `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "lbda = 0.01\n",
    "\n",
    "def pobj_l2(params, X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    return np.mean(huber(y - np.dot(X, w) - b, epsilon=epsilon)) + lbda * np.sum(w ** 2)\n",
    "\n",
    "\n",
    "def huber_lbfgs_l2(X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "# TODO  (for visualization)\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Huber Loss with L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are interested in the $\\ell_1$ regularized model.\n",
    "To help you we give you the code of the objective function to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pobj_l1(params, X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    return np.mean(huber(y - np.dot(X, w) - b, epsilon=epsilon)) + lbda * np.sum(np.abs(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the cost function, you are going to implement solvers based on:\n",
    "\n",
    "- Proximal Gradient Descent (PGD aka ISTA)\n",
    "- Accelerated Proximal Gradient Descent (APGD aka FISTA)\n",
    "- Proximal Coordinate Descent (PCD)\n",
    "\n",
    "Before this we are going to define the `monitor` class previously used in the second lab as well as plotting functions useful to monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monitor(object):\n",
    "    def __init__(self, algo, obj, x_min, args=()):\n",
    "        self.x_min = x_min\n",
    "        self.algo = algo\n",
    "        self.obj = obj\n",
    "        self.args = args\n",
    "        if self.x_min is not None:\n",
    "            self.f_min = obj(x_min, *args)\n",
    "\n",
    "    def run(self, *algo_args, **algo_kwargs):\n",
    "        t0 = time.time()\n",
    "        _, x_list = self.algo(*algo_args, **algo_kwargs)\n",
    "        self.total_time = time.time() - t0\n",
    "        self.x_list = x_list\n",
    "        if self.x_min is not None:\n",
    "            self.err = [linalg.norm(x - self.x_min) for x in x_list]\n",
    "            self.obj = [self.obj(x, *self.args) - self.f_min for x in x_list]\n",
    "        else:\n",
    "            self.obj = [self.obj(x, *self.args) for x in x_list]\n",
    "\n",
    "\n",
    "def plot_epochs(monitors, solvers):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    for monit in monitors:\n",
    "        ax1.semilogy(monit.obj, lw=2)\n",
    "        ax1.set_title(\"Objective\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        if monit.x_min is None:\n",
    "            ax1.set_ylabel(\"$f(x_k)$\")\n",
    "        else:\n",
    "            ax1.set_ylabel(\"$f(x_k) - f(x^*)$\")\n",
    "\n",
    "    ax1.legend(solvers)\n",
    "\n",
    "    for monit in monitors:\n",
    "        if monit.x_min is not None:\n",
    "            ax2.semilogy(monit.err, lw=2)\n",
    "            ax2.set_title(\"Distance to optimum\")\n",
    "            ax2.set_xlabel(\"Epoch\")\n",
    "            ax2.set_ylabel(\"$\\|x_k - x^*\\|_2$\")\n",
    "\n",
    "    ax2.legend(solvers)\n",
    "\n",
    "\n",
    "def plot_time(monitors, solvers):\n",
    "    for monit in monitors:\n",
    "        objs = monit.obj\n",
    "        plt.semilogy(np.linspace(0, monit.total_time, len(objs)), objs, lw=2)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"Timing\")\n",
    "        plt.ylabel(\"$f(x_k) - f(x^*)$\")\n",
    "\n",
    "    plt.legend(solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 9a:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the proximal gradient descent (PGD) method.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Note:**  The parameter `step` is the size of the gradient step that you will need to propose by computing the Lipschitz constant of the data fitting term (Huber term without regularization term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd(x_init, grad, prox, step, n_iter=100, store_every=1,\n",
    "        grad_args=(), prox_args=()):\n",
    "    \"\"\"Proximal gradient descent algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_init : array, shape (n_parameters,)\n",
    "        Parameters of the optimization problem.\n",
    "    grad : callable\n",
    "        The gradient of the smooth data fitting term.\n",
    "    prox : callable\n",
    "        The proximal operator of the regularization term.\n",
    "    step : float\n",
    "        The size of the gradient step done on the smooth term.\n",
    "    n_iter : int\n",
    "        The number of iterations.\n",
    "    store_every : int\n",
    "        At which frequency should the current iterated be remembered.\n",
    "    grad_args : tuple\n",
    "        Parameters to pass to grad.\n",
    "    prox_args : tuple\n",
    "        Parameters to pass to prox.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : array, shape (n_parameters,)\n",
    "        The estimated parameters.\n",
    "    x_list : list\n",
    "        The list if x values along the iterations.\n",
    "    \"\"\"\n",
    "    x = x_init.copy()\n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 9b:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the L1 and L2 proximal operators. You will pay attention to the intercept.\n",
    "    </li>\n",
    "    <li>\n",
    "        Using the monitor class and the plot_epochs function, display the convergence.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "In order to get a good value of `x_min` you will let your PGD solver run for 10000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will need to implement the proximal operator functions for $\\ell_1$ and $\\ell_2$ regularized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_R2(params, reg=1.):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "\n",
    "def prox_R1(params, reg=1.):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "\n",
    "def prox_l2(params, step, lbda):\n",
    "    return prox_R2(params, reg=step * lbda)\n",
    "\n",
    "\n",
    "def prox_l1(params, step, lbda):\n",
    "    return prox_R1(params, reg=step * lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bigger data\n",
    "X, y = make_regression(n_samples=500, n_features=100, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Set initial values of parameters to optimize\n",
    "x_init = np.zeros(n_features + 1)\n",
    "x_init[0] = np.mean(y)\n",
    "n_iter = 2000\n",
    "lbda = 1.\n",
    "epsilon = 1.\n",
    "\n",
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "# Run PGD\n",
    "monitor_pgd_l2 = monitor(pgd, pobj_l2, x_min, args=(X, y, lbda, epsilon))\n",
    "monitor_pgd_l2.run(x_init, grad_huber_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "monitors = [monitor_pgd_l2]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the $\\ell_1$ regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PGD for L1\n",
    "monitor_pgd_l1 = monitor(pgd, pobj_l1, x_min=None, args=(X, y, lbda, epsilon))\n",
    "monitor_pgd_l1.run(x_init, grad_huber_loss, prox_l1, step, n_iter,\n",
    "                   grad_args=(X, y, epsilon), prox_args=(lbda,))\n",
    "\n",
    "monitors = [monitor_pgd_l1]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 10:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the accelerated proximal gradient descent (APGD) and add this solver to the monitoring plots.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apgd(x_init, grad, prox, step, n_iter=100, store_every=1,\n",
    "         grad_args=(), prox_args=()):\n",
    "    \"\"\"Accelerated proximal gradient descent algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    y = x_init.copy()\n",
    "    t = 1.\n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l2, monitor_apgd_l2]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l1, monitor_apgd_l1]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 11:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the proximal coordinate descent (PCD) and add this solver to the monitoring plots for L1 and L2 regularized models.\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Note:** You are welcome to try to use numba to get reasonable performance but don't spend too much time if you get weird numba errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 12:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Compare the performance of the different solvers for different (simulated) problem sizes.\n",
    "    </li>\n",
    "    <li>\n",
    "        What solver would you recommend for what problem and using what regularization?\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Application\n",
    "\n",
    "You will now apply your solver to an environment dataset. Given 2 features:\n",
    "\n",
    " - LNOxEm log of hourly sum of NOx emission of cars on this motorway in arbitrary units.\n",
    " - sqrtWS Square root of wind speed [m/s].\n",
    "\n",
    "The objective is to predict:\n",
    "\n",
    " - log of hourly mean of NOx concentration in ambient air [ppb] next to a highly frequented motorway\n",
    "\n",
    "**Disclaimer:** This dataset is not huge and regularization makes little sense with so little features but it serves as a simple illustration. Also, don't be surprised if Huber loss offers little to no benefit. Again it's just an illustration.\n",
    "\n",
    "Let's first inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('NOxEmissions.csv', index_col=0).drop(['julday'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, ['LNOxEm', 'sqrtWS']].values\n",
    "y = df['LNOx']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 1], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate our experiment we're going to write a full scikit-learn estimator.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 13:</b>\n",
    "    <ul>\n",
    "    <li>\n",
    "        Implement the `fit` method from the estimator in the next cell\n",
    "    </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "class HuberRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"scikit-learn estimator for regression with a Huber loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lbda : float\n",
    "        The regularization parameter\n",
    "    penalty : 'l1' | 'l2'\n",
    "        The type of regularization to use.\n",
    "    max_iter : int\n",
    "        The number of iterations / epochs to do on the data.\n",
    "    solver : 'pgd' | 'apgd' | 'pcd'\n",
    "        The type of algorithm to use.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : ndarray, (n_features,)\n",
    "        The weitghs w.\n",
    "    intercept_ : float\n",
    "        The intercept or bias term b.\n",
    "    \"\"\"\n",
    "    def __init__(self, lbda=1., penalty='l2', epsilon=1.,\n",
    "                 max_iter=2000, solver='pgd'):\n",
    "        self.lbda = lbda\n",
    "        self.penalty = penalty\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "        assert epsilon > 0.\n",
    "        assert self.penalty in ['l1', 'l2']\n",
    "        assert self.solver in ['pgd', 'apgd', 'pcd'] \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The target.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO\n",
    "\n",
    "        # END TODO\n",
    "        self.params_ = x\n",
    "        self.coef_ = x[1:]\n",
    "        self.intercept_ = x[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            The predicted target.\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.coef_) + self.intercept_\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Score using negative mean absolute error\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The target.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            The negative mean absolute error.\n",
    "            Negative to keep the semantic that higher is better.\n",
    "        \"\"\"\n",
    "        return -np.mean(np.abs(y - self.predict(X)))\n",
    "\n",
    "\n",
    "for solver in ['pgd', 'apgd', 'pcd']:\n",
    "    clf = HuberRegressor(lbda=1., penalty='l2', max_iter=1000, solver=solver)\n",
    "    clf.fit(X, y)\n",
    "    print('Solver with L2: %s   \\t-   MAE : %.5f' % (solver, -clf.score(X, y)))\n",
    "\n",
    "for solver in ['pgd', 'apgd', 'pcd']:\n",
    "    clf = HuberRegressor(lbda=1., penalty='l1', max_iter=1000, solver=solver)\n",
    "    clf.fit(X, y)\n",
    "    print('Solver with L1: %s   \\t-   MAE : %.5f' % (solver, -clf.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 14:</b>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Compare the cross-validation performance of your model (using `cross_val_score`) with a Ridge or Lasso regression models using as scoring metric the \"mean absolute error\" (MAE).\n",
    "        </li>\n",
    "        <li>\n",
    "            You will check that the Huber model matches Ridge when epsilon is large. Pay attention to how the loss is scaled in scikit-learn for Ridge (no normalization by 1/n_samples).\n",
    "        </li>\n",
    "        <li>\n",
    "            You will comment on the running time of your solver to reach their optimal prediction performance.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "To score your model with MAE using cross_val_score you need to pass as parameter `scoring='neg_mean_absolute_error'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TODO\n",
    "\n",
    "# END TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
